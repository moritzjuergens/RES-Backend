# -*- coding: utf-8 -*-
"""RES.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pilL-yA8I_EpQiuUMDdt-gHFJKOV6NsV

#Unedited KNN Version
###Mit Data Visualization, etc.
"""
from csv import writer

from careerjet_api import CareerjetAPIClient
import pandas as pd
#import numpy as np
#import matplotlib.pyplot as plt
import warnings
#from sklearn.naive_bayes import MultinomialNB
from sklearn.multiclass import OneVsRestClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score
#from pandas.plotting import scatter_matrix
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
#from pathlib import Path
import nltk
from nltk.corpus import stopwords
import string
#from wordcloud import WordCloud
#import seaborn as sns
#from matplotlib.gridspec import GridSpec
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import PyPDF2 
import json
from sklearn.preprocessing import LabelEncoder
#from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
#warnings.filterwarnings('ignore')


from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

resumeDataSet = pd.read_csv('UpdatedResumeDataSet.csv' ,encoding='utf-8')
#resumeDataSet = pd.read_csv('LinkedIn_ProfileDataset_Text_Role_200.csv' ,encoding='utf-8')
row_count = sum(1 for row in resumeDataSet)
numbers_of_rows = 1500
"""####Functions"""

def cleanString(s):
  s = re.sub('http\S+\s*', ' ', s)  # remove URLs
  s = re.sub('RT|cc', ' ', s)  # remove RT and cc
  s = re.sub('#\S+', '', s)  # remove hashtags
  s = re.sub('@\S+', '  ', s)  # remove mentions
  s = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', s)  # remove punctuations
  s = re.sub(r'[^\x00-\x7f]',r' ', s) 
  s = re.sub('\s+', ' ', s)  # remove extra whitespace
  return s
  
def vectorize(requiredText, mf):
  word_vectorizer = TfidfVectorizer(
    sublinear_tf=True,
    stop_words='english',
    max_features=mf)
  word_vectorizer.fit(requiredText)
  WordFeatures = word_vectorizer.transform(requiredText)
  return WordFeatures

def cleanResume(resumeText):
    resumeText = re.sub('http\S+\s*', ' ', resumeText)  # remove URLs
    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc
    resumeText = re.sub('#\S+', '', resumeText)  # remove hashtags
    resumeText = re.sub('@\S+', '  ', resumeText)  # remove mentions
    resumeText = re.sub('[%s]' % re.escape("""!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', resumeText)  # remove punctuations
    resumeText = re.sub(r'[^\x00-\x7f]',r' ', resumeText) 
    resumeText = re.sub('\s+', ' ', resumeText)  # remove extra whitespace
    return resumeText

def pdf_to_json(pdffile,jsonfile="testResume.json"):
        # Opening JSON file
    f = open(jsonfile)
    # Open pdf file
    pdfFileObj = open(pdffile,'rb')

        # Read file
    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)

        # Get total number of pages
    num_pages = pdfReader.numPages

        # Initialize a count for the number of pages
    count = 0

        # Initialize a text empty string variable
    text = ""

        # Extract text from every page on the file
    while count < num_pages:
        pageObj = pdfReader.getPage(count)
        count +=1
        text += pageObj.extractText()
            
        # Convert all strings to lowercase
    text = text.lower()

        # Remove numbers
    text = re.sub(r'\d+','',text)

        # Remove punctuation
    #text = text.translate(str.maketrans('','',string.punctuation))  
    #text = text.replace('\n','')
    text =  cleanResume(text)
    # returns JSON object as 
    # a dictionary
    data = json.load(f)
    
    # Iterating through the json
    # list
    ab = text.find("skills")
    data[0]['Resume'] = text[ab:]
    #data[0]['Resume'] = text[100:]

    with open(jsonfile, 'w') as f:
        json.dump(data, f)
    
    # Closing file
    f.close()
    return text


def run(jsonfile="testResume.json"):   

    global resumeDataSet
    global numbers_of_rows
    resumeDataSet['cleaned_resume'] = ''
    testDataSet = pd.read_json('testResume.json')
    testDataSet['cleaned_resume']=''
    resumeDataSet.head()
    testDataSet.head()
    targetCounts = resumeDataSet['Category'].value_counts()
    targetLabels  = resumeDataSet['Category'].unique()
    


    resumeDataSet['cleaned_resume'] = resumeDataSet.Resume.apply(lambda x: cleanString(x))
    testDataSet['cleaned_resume'] = testDataSet.Resume.apply(lambda x: cleanString(x))
    var_mod = ['Category']
    le = LabelEncoder()
    for i in var_mod:
        resumeDataSet[i] = le.fit_transform(resumeDataSet[i])

    requiredText = resumeDataSet['cleaned_resume'].values
    requiredText2 = testDataSet['cleaned_resume'].values
    requiredTarget = resumeDataSet['Category'].values
    WordFeatures = vectorize(requiredText, 200)
    WordFeatures2 = vectorize(requiredText2, 200)

    #Train the model
    X_train,X_test,y_train,y_test = train_test_split(WordFeatures,requiredTarget,random_state=0, test_size=0.2)
    clf = OneVsRestClassifier(KNeighborsClassifier())
    clf.fit(X_train, y_train)

    

    #Predict
    #first we use the test data to validate our model
    prediction = clf.predict(X_test)
    #print('Accuracy of KNeighbors Classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))
    #print('Accuracy of KNeighbors Classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))
    #print("\n Classification report for classifier %s:\n%s\n" % (clf, metrics.classification_report(y_test, prediction)))



    #then we can use a singular x value and let the model predict the label for us
    realPrediction = clf.predict(WordFeatures2)
    return(targetLabels[realPrediction][0])
   
    











def get_job():
    keyword = run()


    cj  =  CareerjetAPIClient("en_GB")

    result_json = cj.search({
                            'location'    : 'berlin',
                            'keywords'    : keyword,
                            'affid'       : '213e213hd12344552',
                            'user_ip'     : '11.22.33.44',
                            'url'         : 'http://www.example.com/jobsearch?q=python&l=berlin',
                            'user_agent'  : 'Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Firefox/31.0'
                        })
    result_json["category"] = keyword
    return result_json
        
    

       

def cleaning_json(jsonfile="testResume.json"):
        # Opening JSON file
    f = open(jsonfile)
    

    # returns JSON object as 
    # a dictionary
    data = json.load(f)
    
    # Iterating through the json
    # list
    data[0]['Resume'] = ""

    with open(jsonfile, 'w') as f:
        json.dump(data, f)
    
    # Closing file
    f.close()




def correct_result(pdf_file, category,jsonfile="testResume.json" ):
    #s = '"{}"'.format(str(pdf_to_json(pdf_file,jsonfile)))
    List = [category, str(pdf_to_json(pdf_file,jsonfile))]
    with open('UpdatedResumeDataSet.csv', 'a', newline='',encoding='utf-8') as f_object:
        writer_object = writer(f_object)
        writer_object.writerow(List)
        f_object.close()

