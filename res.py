# -*- coding: utf-8 -*-
"""RES.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pilL-yA8I_EpQiuUMDdt-gHFJKOV6NsV

"""
# libraries
import ssl
import PyPDF2
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import json
from scipy.sparse import hstack
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import re
import string
from nltk.corpus import stopwords
from csv import writer
from careerjet_api import CareerjetAPIClient
import pandas as pd
import warnings
from sklearn.multiclass import OneVsRestClassifier
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
import nltk


# dataset
resumeDataSet = pd.read_csv('ResumeDataSet.csv', encoding='utf-8')
numbers_of_rows = 1500
"""####Functions"""


def cleanString(s):
    s = re.sub('http\S+\s*', ' ', s)  # remove URLs
    s = re.sub('RT|cc', ' ', s)  # remove RT and cc
    s = re.sub('#\S+', '', s)  # remove hashtags
    s = re.sub('@\S+', '  ', s)  # remove mentions
    s = re.sub('[%s]' % re.escape(
        """!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', s)  # remove punctuations
    s = re.sub(r'[^\x00-\x7f]', r' ', s)
    s = re.sub('\s+', ' ', s)  # remove extra whitespace
    return s


def vectorize(requiredText,):
    word_vectorizer = TfidfVectorizer(
        sublinear_tf=True,
        stop_words='english',
        max_features=numbers_of_rows)
    word_vectorizer.fit(requiredText)
    WordFeatures = word_vectorizer.transform(requiredText)
    return WordFeatures


def cleanResume(resumeText):
    resumeText = re.sub('http\S+\s*', ' ', resumeText)  # remove URLs
    resumeText = re.sub('RT|cc', ' ', resumeText)  # remove RT and cc
    resumeText = re.sub('#\S+', '', resumeText)  # remove hashtags
    resumeText = re.sub('@\S+', '  ', resumeText)  # remove mentions
    resumeText = re.sub('[%s]' % re.escape(
        """!"#$%&'()*+,-./:;<=>?@[\]^_`{|}~"""), ' ', resumeText)  # remove punctuations
    resumeText = re.sub(r'[^\x00-\x7f]', r' ', resumeText)
    resumeText = re.sub('\s+', ' ', resumeText)  # remove extra whitespace
    return resumeText


def pdf_to_json(pdffile, jsonfile="Resume.json"):
    # Opening JSON file
    f = open(jsonfile)
    # Open pdf file
    pdfFileObj = open(pdffile, 'rb')

    # Read file
    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)

    # Get total number of pages
    num_pages = pdfReader.numPages

    # Initialize a count for the number of pages
    count = 0

    # Initialize a text empty string variable
    text = ""

    # Extract text from every page on the file
    while count < num_pages:
        pageObj = pdfReader.getPage(count)
        count += 1
        text += pageObj.extractText()

        # Convert all strings to lowercase
    text = text.lower()

    # Remove numbers
    text = re.sub(r'\d+', '', text)

    # Remove punctuation
    # text = text.translate(str.maketrans('','',string.punctuation))
    # text = text.replace('\n','')
    text = cleanResume(text)
    # returns JSON object as
    # a dictionary
    data = json.load(f)

    # Iterating through the json
    # list
    ab = text.find("skills")
    data[0]['Resume'] = text[ab:]
    # data[0]['Resume'] = text[100:]

    with open(jsonfile, 'w') as f:
        json.dump(data, f)

    # Closing file
    f.close()
    return text


def run(jsonfile="Resume.json"):

    global resumeDataSet
    global numbers_of_rows


    resumeDataSet['cleaned_resume'] = ''
    resumeDataSet.head()


# targetCounts = resumeDataSet['Category'].value_counts()
# targetLabels  = resumeDataSet['Category'].unique()
# Make square figures and axes
# plt.figure(1, figsize=(25,25))
# the_grid = GridSpec(2, 2)

# cmap = plt.get_cmap('coolwarm')
# colors = [cmap(i) for i in np.linspace(0, 1, 3)]
# plt.subplot(the_grid[0, 1], aspect=1, title='CATEGORY DISTRIBUTION')

# source_pie = plt.pie(targetCounts, labels=targetLabels, autopct='%1.1f%%', shadow=True, colors=colors)
# plt.show()

    resumeDataSet['cleaned_resume'] = resumeDataSet.Resume.apply(
        lambda x: cleanResume(x))

    var_mod = ['Category']
    # var_mod = ['SKILLS']
    # hier geÃ¤ndert
    le = LabelEncoder()
    for i in var_mod:
        resumeDataSet[i] = le.fit_transform(resumeDataSet[i])

    requiredText = resumeDataSet['cleaned_resume'].values
    requiredTarget = resumeDataSet['Category'].values

    word_vectorizer = TfidfVectorizer(
        sublinear_tf=True,
        stop_words='english',
        max_features=numbers_of_rows)
    word_vectorizer.fit(requiredText)
    WordFeatures = word_vectorizer.transform(requiredText)

    # print ("Feature completed .....")

    X_train, X_test, y_train, y_test = train_test_split(
        WordFeatures, requiredTarget, random_state=0, test_size=0.2)
    # print(X_train.shape)
    # print(X_test.shape)

    clf = OneVsRestClassifier(KNeighborsClassifier())
    clf.fit(X_train, y_train)
    prediction = clf.predict(X_test)
    # print(X_test)
    # print('Accuracy of KNeighbors Classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))
    # print('Accuracy of KNeighbors Classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))

    print("\n Classification report for classifier %s:\n%s\n" %
          (clf, metrics.classification_report(y_test, prediction)))

    realPrediction = clf.predict(X_train[5])

    resumeDataSet = pd.read_csv('ResumeDataSet.csv', encoding='utf-8')
    resumeDataSet['cleaned_resume'] = ''
    testDataSet = pd.read_json(jsonfile)
    testDataSet['cleaned_resume'] = ''
    resumeDataSet.head()
    testDataSet.head()

    """####Pre-Process the Dataset"""

    targetCounts = resumeDataSet['Category'].value_counts()
    targetLabels = resumeDataSet['Category'].unique()

    resumeDataSet['cleaned_resume'] = resumeDataSet.Resume.apply(
        lambda x: cleanString(x))
    testDataSet['cleaned_resume'] = testDataSet.Resume.apply(
        lambda x: cleanString(x))

    var_mod = ['Category']
    le = LabelEncoder()
    for i in var_mod:
        resumeDataSet[i] = le.fit_transform(resumeDataSet[i])

    requiredText = resumeDataSet['cleaned_resume'].values
    requiredText2 = testDataSet['cleaned_resume'].values

    WordFeatures = vectorize(requiredText)
    WordFeatures2 = vectorize(requiredText2)
    WordFeatures2.resize(1, numbers_of_rows)


    """####Train the model"""

    X_train, X_test, y_train, y_test = train_test_split(
        WordFeatures, requiredTarget, random_state=0, test_size=0.2)

    clf = OneVsRestClassifier(KNeighborsClassifier())
    clf.fit(X_train, y_train)

    """####Predict

    first we use the test data to validate our model
    """

    prediction = clf.predict(X_test)

    """and then we can use a singular x value and let the model predict the label for us"""

    print('Calculating the metrics...')
    print("Accuracy:{}".format(accuracy_score(y_test, prediction)))
   

    realPrediction = clf.predict(WordFeatures2)
    return(targetLabels[realPrediction[0]])


def prediction():
    return run()

#get the jobs as json fron the api
def get_job(keyword, location):
    #language
    cj = CareerjetAPIClient("en_GB")

    result_json = cj.search({
                            'location': location,
                            'keywords': keyword,
                            'affid': '213e213hd12344552',
                            'user_ip': '11.22.33.44',
                            'url': 'http://www.example.com/jobsearch?q=python&l=berlin',
                            'user_agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Firefox/31.0'
                            })
    result_json["category"] = keyword

    return result_json

#delete all the elements from the json file 
def cleaning_json(jsonfile="Resume.json"):
    # Opening JSON file
    f = open(jsonfile)

    # returns JSON object as
    # a dictionary
    data = json.load(f)

    # Iterating through the json
    # list
    data[0]['Resume'] = ""

    with open(jsonfile, 'w') as f:
        json.dump(data, f)

    # Closing file
    f.close()

#add cv to the dataset 
def correct_result(pdf_file, category, jsonfile="Resume.json"):
    # s = '"{}"'.format(str(pdf_to_json(pdf_file,jsonfile)))
    List = [category, str(pdf_to_json(pdf_file, jsonfile))]
    with open('ResumeDataSet.csv', 'a') as f_object:
        writer_object = writer(f_object)
        writer_object.writerow(List)
        f_object.close()
